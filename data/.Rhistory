setwd("~/Library/CloudStorage/OneDrive-Personal/Documents/Academic/OSU/Git/multivariate-analysis")
library(vegan)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(ggplot2)
source("biostats.R")
#site-level data
#dat <- readRDS("C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/oss-occu/data/site_level_matrix.rds")
dat <- readRDS("~/Library/CloudStorage/OneDrive-Personal/Documents/Academic/OSU/Git/oss-occu/data/site_level_matrix.rds")
row.names(dat) <- dat[,1]
#dat2 <- subset(dat, year=="2024")
sals <- dat[26:27]
env <- dat[1:25]
drop <- c("lat","long","stand","tree_farm","landowner","site_id","year","weather")
env <- env[,!(colnames(env) %in% drop)]
env_cont <- env[,-1]
drop <- c("jul_date","veg_cov","fwd_cov","dwd_count","size_cl","decay_cl","char_cl","length_cl" )
env_subset <- env_cont[,!(colnames(env_cont) %in% drop)]
env_std_subset <- env_std[,c("temp","dwd_cov","soil_moist","stumps","logs","decay_cl","canopy_cov")]
env_std <- decostand(env_cont, "standardize") #Z-scores the data in each column
env_std <- decostand(env_cont, "standardize") #Z-scores the data in each column
env_std_subset <- env_std[,c("temp","dwd_cov","soil_moist","stumps","logs","decay_cl","canopy_cov")]
env_euc <- vegdist(env_std_subset, method="euclidean")
env.pcoa <- cmdscale(env_euc,
k=5,
eig=TRUE,
add=T)
spe.sc <- wascores(env.pcoa$points[,1:2], env_subset)
vec.sp <- envfit(as.data.frame(env.pcoa$points), env_subset, perm=1000)
# plot
groups <- levels(factor(dat$trt))
pt_col <- viridis(length(groups))
site.sc <- scores(env.pcoa, choices=c(1,2))
plot(site.sc[, 1:2],  # First two dimensions
main = "Env PCoA",
xlab = "PCoA 1",
ylab = "PCoA 2",
pch = 19)
for (i in 1:length(groups))
{
dim_choice <- site.sc[dat$trt==groups[i],]
points(dim_choice[,1], dim_choice[,2],
pch=19,
cex=1.4,
col=pt_col[i])
# Calculate and add convex hull for the group
chull_points <- dim_choice[chull(dim_choice[, 1], dim_choice[, 2]), ]
polygon(chull_points[, 1], chull_points[, 2],
border=pt_col[i],
col=adjustcolor(pt_col[i], alpha.f = 0.3),  # Semi-transparent fill
lwd=2)
}
text(spe.sc*1.5, row.names(spe.sc),
cex = 1.5)
arrows(0, 0, spe.sc[,1]*1.4, spe.sc[,2]*1.4,
lwd=2,
length=0.1,
cex= 1.5)
legend(x="bottomleft",
legend=levels(factor(dat$trt)),
col=pt_col[1:6],
pch=19,
cex=1.2)
library(vegan)
library(viridis)
View(vec.sp)
View(spe.sc)
View(env.pcoa)
100*env.pcoa$eig/sum(env.pcoa$eig)
# Extract eigenvalues
eigenvalues <- pcoa_result$eig
# Extract eigenvalues
eigenvalues <- env.pcoa$eig
# Calculate the percentage of variance explained by each axis
percent_explained <- (eigenvalues / sum(eigenvalues)) * 100
# View the percentages for the first few axes
percent_explained[1:5]
rm(list=ls())
#setwd("C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/oss-occu/data")
setwd("~/Library/CloudStorage/OneDrive-Personal/Documents/Academic/OSU/Git/oss-occu/data")
library(nimble)
library(ggplot2)
library(data.table)
library(tidyverse)
library(mcmcplots)
library(MCMCvis)
library(boot)
source('attach.nimble_v2.R')
site <- read.csv("site.complete.csv")
subplot <- read.csv("subplot.complete.csv")
sals <- read.csv("sals.complete.csv",
colClasses = c(landowner="factor", stand="character", trt="factor",
obs="factor", subplot="factor", recap="factor",
pass="factor", spp="factor", cover_obj="factor",
substrate="factor", age_class="factor"))
oss.dets <- read.csv("oss.occu.wide.csv")
enes.dets <- read.csv("enes.occu.wide.csv")
all.long <- read.csv("all.occu.long.csv")
oss.long <- read.csv("oss.occu.long.csv")
enes.long <- read.csv("enes.occu.long.csv")
subplot_subset <- subset(subplot[,c(8,1,9,18)]) #subset subplot info
df1 <- subset(site[,c(1,10,11)]) #subset site info
dfmerge <- full_join(subplot_subset,df1,by="site_id") #merge site and subplot
#prev line joins temp and hum to subplot matrix, which automatically repeats the
#temp and hum in blank subplot cells from same site id for both years because
#its currently only listed for each site
colnames(oss.long)[3] <- "oss.obs"
colnames(enes.long)[3] <- "enes.obs"
colnames(all.long)[3] <- "all.obs"
merge1 <- merge(dfmerge, all.long, by=c("site_id","subplot"))
merge1 <- merge(merge1, oss.long, by=c("site_id","subplot"))
complete.merge <- merge(merge1,enes.long,by=c("site_id","subplot"))
colnames(complete.merge) <- c("site","subplot","date","soil.moist","temp",
"humidity","all.obs","oss.obs","enes.obs")
#### site-level matrix:
# "site" and "treatment
site_subset <- subset(site[,c(1,5)])
site_subset$trt <- as.factor(site_subset$trt)
site_subset$trt <- as.numeric(site_subset$trt)
table(site_subset$trt)
colnames(site_subset)[2] <- "treatment"
#  1 = BS
#  2 = BU
#  3 = HB
#  4 = HU
#  5 = UU
data <- complete.merge
#data$site <- as.numeric(data$site)
data$all.obs <- as.numeric(data$all.obs)
data$oss.obs <- as.numeric(data$oss.obs)
data$enes.obs <- as.numeric(data$enes.obs)
data2 <- site_subset
table(data2$treatment)
#  1 = BS
#  2 = BU
#  3 = HB
#  4 = HU
#  5 = UU
scaled_temp <- c(scale(data$temp))
oss.model <- nimbleCode ({
# Priors
# uninformative, vague priors
for(t in 1:n.treatments){
TreatmentIntercept[t] ~ dunif(-10,10)
}#t
DetectionIntercept ~ dunif(-5,5)
betaTemp ~ dunif(-5, 5)
betaTemp2 ~ dunif(-5, 0)
# Likelihood
# Process/Biological model = Occupancy
# need two pieces: one defining psi(occu prob coeff) and covariates, and one defining z dist
for(i in 1:n.sites) {
logit(psi[i]) <- TreatmentIntercept[treatment[i]]  #psi=occupancy probability
z[i] ~ dbern(psi[i])  # z=1 if occupied, z=latent true occupancy
}#i
# Observation model = Detection
# need two pieces: one for p(det prob coeff) and one defining Y distribution
for(j in 1:n.obs) {
logit(p[j]) <- DetectionIntercept + betaTemp*temp[j] + betaTemp2*temp[j]^2
#using temp as covariate with a quadratic relationship
#p=detection probability for site i and survey j
Y[j] ~ dbern(p[j] * z[site[j]]) #Y=my actual data observations
#z=1 or 0, turns this on or off
}#j
})
# Parameters monitored
parameters <- c("z","p","TreatmentIntercept","DetectionIntercept","betaTemp", "betaTemp2")
# MCMC Settings
ni <- 40000
nt <- 40
nb <- 20000
nc <- 3
# Data
nimble.data = list(Y=data$oss.obs,
temp=scaled_temp)
nimble.constants = list(n.sites = length(unique(data$site)),
n.treatments = length(unique(data2$treatment)),
treatment=data2$treatment,
site=as.numeric(as.factor(data$site)),
n.obs = length(data$all.obs))
mcmc.output.3 <- nimbleMCMC(code = oss.model,
data = nimble.data,
constants=nimble.constants,
monitors = parameters,
niter = ni,
nburnin = nb,
nchains = nc,
thin=nt,
summary=TRUE,
samplesAsCodaMCMC = TRUE)
attach.nimble(mcmc.output.3$samples)
save(mcmc.output.3, file="./oss_model.RData")
load("./oss_model.RData")
# Gelman-Rubin diagnostic (AKA RHat or PSRF)
z <- mcmc.output.3$samples
g <- matrix(NA, nrow=nvar(z), ncol=2)
for (v in 1:nvar(z)) { g[v,] <- gelman.diag(z[,v])$psrf }
PSRF <- bind_cols(colnames(z$chain1),g) %>% rename(Parameter = ...1 ,PSRF = ...2 ,PSRFUpperCI = ...3)
PSRF # Values are below 1.05, so that's good
# Inverse logit the detection intercept to get detection probabilities
det.probs.inv <- inv.logit(DetectionIntercept)
hist(det.probs.inv)
# Looking at trace plots and parameter estimates
MCMCtrace(object = mcmc.output.3$samples,
pdf = FALSE, # no export to PDF
ind = TRUE, # separate density lines per chain
params = c("DetectionIntercept", "betaTemp", "TreatmentIntercept"))
mean(det.probs.inv) # = 0.26108
mean(det.probs.inv>0)  # = 1
median(det.probs.inv)  # = 0.2597028
boxplot(det.probs.inv)
# Inv logit TreatmentIntercept to get Occupancy Estimates
trt.int.inv <- inv.logit(TreatmentIntercept)
median(trt.int.inv[,1]) # 0.6392119    BS
median(trt.int.inv[,2]) # 0.8090534    BU
median(trt.int.inv[,3]) # 0.5784290    HB
median(trt.int.inv[,4]) # 0.6915276    HU
median(trt.int.inv[,5]) # 0.9920444    UU
apply(trt.int.inv[,1], 2, quantile, probs = c(0.025, 0.975))  # Example for BS
quantile(trt.int.inv[,1], probs = c(0.025, 0.975))  # For BS
quantile(trt.int.inv[,2], probs = c(0.025, 0.975))  # For BU
quantile(trt.int.inv[,3], probs = c(0.025, 0.975))  # For HB
quantile(trt.int.inv[,4], probs = c(0.025, 0.975))  # For HU
quantile(trt.int.inv[,5], probs = c(0.025, 0.975))  # For UU
CI_BS <- quantile(trt.int.inv[,1], probs = c(0.025, 0.975))  # For BS 0.3971932 0.9028864
CI_BU <- quantile(trt.int.inv[,2], probs = c(0.025, 0.975))  # For BU 0.5463141 0.9998092
CI_HB <- quantile(trt.int.inv[,3], probs = c(0.025, 0.975))  # For HB 0.3422422 0.8158058
CI_HU <- quantile(trt.int.inv[,4], probs = c(0.025, 0.975))  # For HU 0.4646734 0.9109540
CI_UU <- quantile(trt.int.inv[,5], probs = c(0.025, 0.975))  # For UU 0.7595295 0.9999382
trt.int.inv <- inv.logit(TreatmentIntercept)
treatment_matrix <- trt.int.inv # Using the inv logit treatment estimates
new.names <- c("Salvage Logged", "Wildfire", "Harvest, Wildfire", "Harvest", "Control")
colnames(treatment_matrix) <- new.names
desired.order <- c("Control", "Wildfire", "Harvest, Wildfire", "Harvest", "Salvage Logged")
box.colors <- c('lightgreen','steelblue', 'coral2', '#f9d62e', '#b967ff' )
#boxplot of Treatment Estimates - occu prob for each treatment
boxplot(treatment_matrix[, match(desired.order, colnames(treatment_matrix))],
main = "Treatment Intercepts for All Species",
xlab = "Treatment", ylab = "Occupancy Probability",
col = box.colors)
